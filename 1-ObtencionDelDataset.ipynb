{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b279873-9b42-4d7e-b020-b0b4b4f8d936",
   "metadata": {},
   "source": [
    "### En el presente notebook se presenta el código utilizado para la generación del dataset con el que se trabajará durante gran parte del trabajo.\n",
    "### El mismo está compuesto por 10.000 Tweets recolectados que contienen la palabra hamburguesa. En un comienzo, el informe intentó hacerse con los pertenecientes a Uruguay, pero el volumen fue demasiado pequeño como para obtener buenos resultados.\n",
    "### Dicho dataset se encuentra en el archivo `tweets_worldwide_hamburguesa.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_key = ''\n",
    "access_secret = ''\n",
    "\n",
    "OAUTH_KEYS = {'consumer_key':consumer_key, 'consumer_secret':consumer_secret, 'access_token_key':access_key, 'access_token_secret':access_secret}\n",
    "auth = tweepy.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-cocktail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"hamburguesa\" + \" -filter:retweets\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tweepy.Cursor(api.search, q=query).items(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_names = []\n",
    "texts = []\n",
    "dates = []\n",
    "langs = []\n",
    "for tweet in results:\n",
    "    screen_names.append(tweet.user.screen_name)\n",
    "    texts.append(tweet.text)\n",
    "    dates.append(tweet.created_at)\n",
    "    langs.append(tweet.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(screen_names))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['screen_names'] = screen_names\n",
    "df['texts'] = texts\n",
    "df['dates'] = dates\n",
    "df['langs'] = langs\n",
    "df.to_csv('tweets_worldwide_hamburguesa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis",
   "language": "python",
   "name": "analisis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
